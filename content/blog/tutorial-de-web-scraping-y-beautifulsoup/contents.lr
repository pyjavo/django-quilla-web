title: Tutorial: Web Scraping y BeautifulSoup
---
author: javier-daza
---
body:

Siempre tuve en mis favoritos del navegador el tutorial de dataquest sobre web scraping con beautifulsoup por Alex Olteanu, pero desde hace un tiempo para ac√°, bajaron dicho art√≠culo de su p√°gina web. El prop√≥sito de este art√≠culo es rescatar ese contenido y, de paso, traducirlo al espa√±ol üí™.

¬°Empecemos!

Para obtener datos para proyectos de ciencia de datos, a menudo confiar√° en bases de datos [SQL](https://es.wikipedia.org/wiki/SQL) y [NoSQL](https://es.wikipedia.org/wiki/NoSQL), [API](https://es.wikipedia.org/wiki/API) o conjuntos de datos en formato CSV listos para usar.

El problema es que no siempre puede encontrar un conjunto de datos sobre un tema espec√≠fico, las bases de datos no se mantienen actualizadas y las API son costosas o tienen l√≠mites de uso.

Sin embargo, si los datos que est√° buscando est√°n en una p√°gina web, entonces la soluci√≥n a todos estos problemas es el **web scraping.**

En este tutorial, aprenderemos a raspar varias p√°ginas web con Python usando [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) y [requests](https://docs.python-requests.org/en/latest/). Luego realizaremos un an√°lisis simple usando [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) y [matplotlib](https://matplotlib.org/).

Para seguir este art√≠culo necesitas tener:
- Una comprensi√≥n b√°sica de HTML.
- Una buena comprensi√≥n de los conceptos b√°sicos de Python.
- Una idea aproximada de qu√© es el web scraping.


## Scraping de datos para m√°s de 2000 pel√≠culas


Queremos analizar las distribuciones de las clasificaciones de pel√≠culas de [IMDB](https://www.imdb.com/) y [Metacritic](https://www.metacritic.com/) para ver si encontramos algo interesante. Para hacer esto, primero recopilaremos datos de m√°s de 2000 pel√≠culas.

<span style="display: block;font-weight: 800;font-size: 1.7rem;font-style: italic;margin: 2rem 0;">
    Es fundamental identificar el objetivo de nuestro scraping desde el principio.
</span>

Escribir un script de scraping puede llevar mucho tiempo, especialmente si queremos scrapear m√°s de una p√°gina web. Queremos evitar pasar horas escribiendo un script que extrae datos que en realidad no necesitaremos.

## Determinando qu√© p√°ginas vamos a scrapear

Una vez que hemos establecido nuestro objetivo, debemos identificar un conjunto eficiente de p√°ginas para scrapear.

Queremos encontrar una combinaci√≥n de p√°ginas que requiera un n√∫mero relativamente peque√±o de solicitudes. Una solicitud (*request* en ingl√©s) es lo que sucede cada vez que accedemos a una p√°gina web. Nosotros 'solicitamos' el contenido de una p√°gina del servidor. Cuantas m√°s solicitudes hagamos, m√°s tiempo necesitar√° ejecutarse nuestro script y mayor ser√° la sobrecarga en el servidor.

Una forma de obtener todos los datos que necesitamos es compilar una lista de nombres de pel√≠culas y usarla para acceder a la p√°gina web de cada pel√≠cula en los sitios web de IMDB y Metacritic.


![Busqueda pelicula](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/busqueda_pelicula.gif)


Como queremos obtener m√°s de 2000 calificaciones tanto de IMDB como de Metacritic, tendremos que realizar al menos 4000 solicitudes. Si hacemos una solicitud por segundo, nuestro script necesitar√° un poco m√°s de una hora para realizar 4000 solicitudes. Por ello, merece la pena intentar identificar formas m√°s eficientes de obtener nuestros datos.

Si exploramos el sitio web de IMDB, podemos descubrir una forma de reducir a la mitad el n√∫mero de solicitudes. Las puntuaciones de Metacritic se muestran en la p√°gina de pel√≠culas de IMDB, por lo que podemos eliminar ambas clasificaciones con una sola solicitud:


![pel√≠cula Oppenheimer](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/oppenheimer_puntuaciones.png)

Si investigamos m√°s a fondo el sitio de IMDB, podemos descubrir la p√°gina que se muestra a continuaci√≥n. Contiene todos los datos que necesitamos para cierto n√∫mero de pel√≠culas (Por defecto 50, pero podemos escoger 250). Dado nuestro objetivo, esto significa que solo tendremos que hacer un n√∫mero limitado de solicitudes (alrededor de 46), lo que es 100 veces menos que nuestra primera opci√≥n. Exploremos m√°s esta √∫ltima opci√≥n.


![Puntuaciones peliculas 2003](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/2023_puntuaciones.png)


## Identificando la estructura de la URL


Nuestro desaf√≠o ahora es asegurarnos de que entendemos la l√≥gica de la URL a medida que cambian las p√°ginas que queremos scrapear. Si no podemos entender esta l√≥gica lo suficiente como para poder implementarla en el c√≥digo, llegaremos a un callej√≥n sin salida.

Si accede a la p√°gina de [b√∫squeda avanzada](https://www.imdb.com/search/) de IMDB, y selecciona `advance title search` puede buscar pel√≠culas por [a√±o](https://www.imdb.com/search/title/#releasedate):

![B√∫squeda avanzada](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/busqueda_avanzada.png)

En el campo "Release Date", busquemos por fechas de 2022-01-01 a 2022-12-31. Luego, al final de la p√°gina le damos click al bot√≥n de buscar. En la siguiente p√°gina ordenamos las pel√≠culas por n√∫mero de votos y le damos click al bot√≥n de siguiente, al final de la p√°gina final. As√≠ llegaremos a esta p√°gina web, que tiene esta URL:


![Estructura URL](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/estructura_url.png)

En la imagen de arriba, puedes ver que la URL tiene varios par√°metros despu√©s del signo de interrogaci√≥n:

- `release_date`: muestra solo las pel√≠culas estrenadas en un a√±o espec√≠fico.
- `sort`: Ordena las pel√≠culas en la p√°gina. `sort=num_votes,desc se traduce en ordenar por n√∫mero de votos en orden descendente.
- `start`: especifica el n√∫mero de inicio de las p√°ginas a listar.
- `ref_`: Nos lleva a la p√°gina siguiente o anterior. La referencia es la p√°gina en la que nos encontramos actualmente. `adv_nxt` y `adv_prv` son dos valores posibles. Se traducen para avanzar a la p√°gina siguiente y avanzar a la p√°gina anterior, respectivamente.


Si navega por esas p√°ginas y observa la URL, notar√° que solo cambian los valores de los par√°metros. Esto significa que podemos escribir un script para que coincida con la l√≥gica de los cambios y hacer muchas menos solicitudes para escrapear nuestros datos.

Comencemos a escribir el script solicitando el contenido de esta √∫nica p√°gina web:

```python
url = "https://www.imdb.com/search/title/?release_date=2023&count=250"
```

En el primer bloque de c√≥digo vamos a:

- Instalar `requests`

```bash
pip install requests
```
- Importar la funci√≥n `get()` desde el m√≥dulo de `requests`.
- Asignar la direcci√≥n de la p√°gina web a una variable llamada `url`.
- Solicitar al servidor el contenido de la p√°gina web utilizando `get()` y almacenar la respuesta del servidor en la variable `respuesta`.
- Imprima una peque√±a parte del contenido de la `respuesta` accediendo a su atributo `.text` (`respuesta` ahora es un objeto `Response`).


```python
from requests import get

url = 'https://www.imdb.com/search/title/?release_date=2023&count=250'

respuesta = get(url)

if respuesta.status_code == 200:
    print(respuesta.text[:500])
```

La respuesta que se obtiene:

```html
<!DOCTYPE html>
<html
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="http://www.facebook.com/2008/fbml">
    <head>
        <meta charset="utf-8">
        <script type="text/javascript">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script>
<script>
    if (typeof uet == 'function') {
      uet("bb", "LoadTitle", {wb: 1});
    }
</script>
  <script>(function(t){ (t.events = t.events || {})["csm_head_pre_title"] = new Date().getTime(); })(IMDbTimer);</script>
   
```

## Comprendiendo la estructura HTML de una sola p√°gina

Como puede ver en la primera l√≠nea de `respuesta.text`, el servidor nos envi√≥ un documento HTML. Este documento describe la estructura general de esa p√°gina web, junto con su contenido espec√≠fico (que es lo que hace que esa p√°gina en particular sea √∫nica).

Todas las p√°ginas que queremos escrapear tienen la misma estructura general. Esto implica que tambi√©n tienen la misma estructura general de HTML. Entonces, para escribir nuestro script, bastar√° con comprender la estructura HTML de una sola p√°gina. Para hacer eso, usaremos **las herramientas de desarrollo del navegador** (Developer Tools en ingl√©s).

Si usa [Chrome](https://developer.chrome.com/devtools) o Brave, haga clic con el bot√≥n derecho en un elemento de la p√°gina web que le interese y luego haga clic en *Inspeccionar*. Esto lo llevar√° directamente a la l√≠nea HTML que corresponde a ese elemento:

![Inspeccionar elementos](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/inspeccionar_elementos.png)

Tambi√©n puede hacer esto con [Firefox](https://developer.mozilla.org/es/docs/Learn/Common_questions/Tools_and_setup/What_are_browser_developer_tools) o [Safari](https://developer.apple.com/safari/tools/) DevTools.

Tenga en cuenta que toda la informaci√≥n de cada pel√≠cula, incluido el p√≥ster, est√° contenida en una etiqueta `div`.


![Etiqueta div](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/div.png)


Hay muchas l√≠neas HTML anidadas dentro de cada etiqueta `div`. Puede explorarlos haciendo clic en esas peque√±as flechas grises a la izquierda de las l√≠neas HTML correspondientes a cada `div`. Dentro de estas etiquetas anidadas encontraremos la informaci√≥n que necesitamos, como la calificaci√≥n de una pel√≠cula.


![Puntuaci√≥n](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/puntuacion.png)

Se muestran 50 pel√≠culas por p√°gina, por lo que debe haber un contenedor `div` para cada una. Extraigamos todos estos 50 contenedores mediante el an√°lisis gramatical (*parsing* en ingl√©s)del documento HTML de nuestra solicitud anterior.

## Usando BeautifulSoup para analizar el contenido HTML

Para analizar nuestro documento HTML y extraer los contenedores de 50 `div, usaremos un m√≥dulo de Python llamado [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/), el m√≥dulo de web scraping m√°s com√∫n para Python.

En el siguiente bloque de c√≥digo vamos a:

- Instalar `BeautifulSoup`.
- Importar el creador de la clase `BeautifulSoup` del paquete `bs4`.
- *Parsear* `respuesta.text` al crear un objeto `BeautifulSoup` y asignar este objeto a `sopa_html`. El argumento `'html.parser'` indica que queremos realizar el an√°lisis utilizando el [analizador HTML integrado de Python](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use).


```bash
pip install beautifulsoup4
```

```python
from bs4 import BeautifulSoup
sopa_html = BeautifulSoup(respuesta.text, 'html.parser')
type(sopa_html)
```

Esto imprime que nuestra sopa es un objeto de tipo:

```bash
<class 'bs4.BeautifulSoup'>
```

Antes de extraer los 50 contenedores `div`, debemos averiguar qu√© los distingue de otros elementos `div` en esa p√°gina. A menudo, la marca distintiva reside en el [atributo](https://www.w3schools.com/Tags/att_global_class.asp) de `class`. Si inspecciona las l√≠neas HTML de los contenedores de inter√©s, notar√° que el atributo de `class` tiene dos valores:

- `lister-item`
- `mode-advanced`.

Esta combinaci√≥n es exclusiva de estos contenedores `div`. Podemos ver que es cierto haciendo una b√∫squeda r√°pida con `(Ctrl + F)`. Tenemos 50 contenedores de este tipo, por lo que esperamos ver solo 50 coincidencias:

<img src="/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/div_50.png" alt="50 coincidencias" title="50 coincidencias" style="width:60%;margin: auto;display: block;" /> 

Ahora usemos el m√©todo `find_all()` para extraer todos los contenedores `div` que tienen un atributo de clase de tipo `lister-item mode-advanced`:

```python
contenedores_pelicula = sopa_html.find_all('div', class_ = 'lister-item mode-advanced')
print('Contenedores de la pel√≠cula:', type(contenedores_pelicula))
print('Longitud:', len(contenedores_pelicula))
```

Lo cual imprime

```bash
Contenedores de la pel√≠cula: <class 'bs4.element.ResultSet'>
Longitud: 250
```

`find_all` devolvi√≥ un objeto tipo `ResultSet` que contiene una lista de los 250 divs que nos interesan.

Ahora solo seleccionaremos el primer contenedor y, por turno, extraeremos cada elemento que nos interesa:
- El nombre de la pelicula
- El a√±o del estreno de la pel√≠cula
- La calificaci√≥n de IMDB
- El Metascore
- El n√∫mero de votos

*----- Imagen -------*

## Extraer los datos de una sola pel√≠cula

Podemos acceder al primer contenedor, que contiene informaci√≥n sobre una sola pel√≠cula, usando la notaci√≥n de lista en `contenedores_pelicula`.

```python
primera_pelicula = contenedores_pelicula[0]
print(primera_pelicula)
```

*----- HTML de respuesta -------*

Esto nos regresa un HTML bastante largo, por lo que para hayar cada elemento, necesitaremos usar el inspector de elementos del navegador.


### El nombre de la pelicula

Comenzamos con el nombre de la pel√≠cula y localizamos su l√≠nea HTML correspondiente usando el inspector de elementos. Puede ver que el nombre est√° contenido dentro de una etiqueta de anclaje (`<a>`). Esta etiqueta est√° anidada dentro de una etiqueta de encabezado (`<h3>`). La etiqueta `<h3>` est√° anidada dentro de una etiqueta `<div>`. Este `<div>` es el tercero de los divs anidados en el contenedor de la primera pel√≠cula. Nosotros ya almacenamos el contenido de este contenedor en la variable `primera_pelicula`.

*----- Imagen -------*

`primera_pelicula` es un objeto `Tag` y las diversas etiquetas HTML que contiene se almacenan como sus atributos. Podemos acceder a ellos tal como acceder√≠amos a cualquier atributo de un objeto Python. Sin embargo, usar un nombre de etiqueta como atributo solo seleccionar√° la primera etiqueta con ese nombre. Si ejecutamos `primera_pelicula.div`, solo obtenemos el contenido de la primera etiqueta `div`:


```bash
print(primera_pelicula.div)
```

```html
<div class="lister-top-right">
<div class="ribbonize" data-caller="filmosearch" data-tconst="tt14452776"></div>
</div>
```
Accediendo a la primera etiqueta de anclaje (`<a>`) no nos lleva al nombre de la pel√≠cula. El primer `<a>` est√° en alg√∫n lugar dentro del segundo div:

```bash
print(primera_pelicula.a)
```

```html
<a href="/title/tt14452776/"> <img alt="The Bear" class="loadlate" data-tconst="tt14452776" height="98" loadlate="https://m.media-amazon.com/images/M/MV5BYmM4MjBkNGMtZjE5Zi00ZDMwLWE5MjYtN2M0MTM2YTQ2MmNlXkEyXkFqcGdeQXVyMjkwOTAyMDU@._V1_UX67_CR0,0,67,98_AL_.jpg" src="https://m.media-amazon.com/images/S/sash/4FyxwxECzL-U1J8.png" width="67"/>
</a>
```

Sin embargo, accediendo a la primera etiqueta `<h3>` nos acerca mucho:

```bash
print(primera_pelicula.h3)
```

```html
<h3 class="lister-item-header">
<span class="lister-item-index unbold text-primary">1.</span>
<a href="/title/tt14452776/">The Bear</a>
<span class="lister-item-year text-muted unbold">(2022‚Äì )</span>
</h3>
```

Desde aqu√≠, podemos usar la notaci√≥n de atributos para acceder al primer `<a>` dentro de la etiqueta `<h3>`:


```bash
print(primera_pelicula.h3.a)
```

```html
<a href="/title/tt14452776/">The Bear</a>
```

Ahora es s√≥lo cuesti√≥n de acceder al texto desde esa etiqueta `<a>:


```python
primer_nombre = primera_pelicula.h3.a.text
print(primer_nombre)
```

```bash
'The Bear'
```

### El a√±o del estreno de la pel√≠cula
Este dato est√° en una etiqueta `<span>` que se encuentra debajo de la etiqueta `<a>` que contiene el nombre.

*----- Imagen -------*

La notaci√≥n de puntos solo acceder√° al primer elemento `span`. Buscaremos por la marca distintiva del segundo `<span>`. Usaremos el m√©todo `find()` que es casi igual que `find_all()`, excepto que solo devuelve la primera coincidencia. De hecho, `find()` es equivalente a `find_all(limit = 1)`. El argumento `limit` limita la salida a la primera coincidencia.

La marca distintiva consiste en los valores `lister-item-year text-muted unbold` asignados al atributo `class`. Entonces buscamos el primer `<span>` con estos valores dentro de la etiqueta `<h3>`:


```python
primer_a√±o = primera_pelicula.h3.find('span', class_ = 'lister-item-year text-muted unbold')
print(primer_a√±o)
```

```bash
<span class="lister-item-year text-muted unbold">(2022‚Äì )</span>
```

Desde aqu√≠, simplemente accedemos al texto usando notaci√≥n de atributos:

```python
primer_a√±o = primer_a√±o.text
print(primer_a√±o)
```

```bash
'(2022‚Äì )'
```

Podr√≠amos limpiar f√°cilmente esa salida y convertirla a un n√∫mero entero. Pero si exploras m√°s p√°ginas, notar√°s que para algunas pel√≠culas el a√±o toma valores impredecibles como (2017)(I) o (2015)(V). Es m√°s eficiente hacer la limpieza despu√©s del scrapeado, cuando conoceremos todos los valores del a√±o.


### La calificaci√≥n de IMDB
Ahora nos centramos en extraer la calificaci√≥n IMDB de la primera pel√≠cula.

Hay un par de formas de hacerlo, pero primero probaremos la m√°s sencilla. Si inspecciona la calificaci√≥n de IMDB usando DevTools, notar√° que la calificaci√≥n est√° contenida dentro de una etiqueta `<strong>`.

*----- Imagen -------*

Usemos notaci√≥n de atributos y esperemos que el primer `<strong>` tambi√©n sea el que contenga la calificaci√≥n.

```python
print(primera_pelicula.strong)
```

```html
<strong>8.5</strong>
```

¬°Excelente! Accederemos al texto, lo convertiremos al tipo `float` y lo asignaremos a la variable `primer_imdb`:

```python
primer_imdb = float(primera_pelicula.strong.text)
print(primer_imdb)
```

```bash
8.5
```


### El Metascore

Si inspeccionamos el Metascore usando inspectod de elementos, notaremos que podemos encontrarlo dentro de una etiqueta `span`.

*----- Imagen -------*

La notaci√≥n de atributos claramente no es una soluci√≥n. Hay muchas etiquetas `<span>` antes de eso. Puedes ver una justo encima de la etiqueta `<strong>`. Ser√° mejor que utilicemos los valores distintivos del atributo `class` (`metascore favorable`).

*Tenga en cuenta que si copia y pega esos valores de la pesta√±a del inspector de elemebtos, habr√° dos caracteres de espacio en blanco entre `metascore` y `favorable`. Aseg√∫rese de que solo haya un car√°cter de espacio en blanco cuando pase los valores como argumentos al par√°metro `class_`. De lo contrario, `find()` no encontrar√° nada.*


```python
primer_mscore = primera_pelicula.find('span', class_ = 'metascore favorable')
primer_mscore = int(primer_mscore.text)
print(primer_mscore)
```

```bash
8.5
```

No todas las pel√≠culas tienen puntuaci√≥n. M√°s adelante abordaremos ese asunto.

Por ahora veamos que el valor `favorable` indica un Metascore alto y establece el color de fondo de la calificaci√≥n en verde. Los otros dos valores posibles son `unfavorable` y mixed`. Sin embargo, lo que es espec√≠fico de todas las calificaciones de Metascore es solo el valor de `metascore`. Este es el que usaremos cuando escribamos el script para toda la p√°gina.


### El n√∫mero de votos
El n√∫mero de votos est√° contenido en una etiqueta `<span>`. Su signo distintivo es un atributo del `name` con el valor `nv`.

*----- Imagen -------*

El atributo de `name` es diferente del atributo de `class`. Usando BeautifulSoup podemos acceder a elementos por cualquier atributo. Las funciones `find()` y `find_all()` tienen un par√°metro llamado `attrs`. A esto podemos pasarle los atributos y valores que estamos buscando como diccionario:


```python
primeros_votos = primera_pelicula.find('span', attrs = {'name':'nv'})
print(primeros_votos)
```

```html
<span data-value="23878" name="nv">23,878</span>
```

Podr√≠amos usar la notaci√≥n `.text` para acceder al contenido de la etiqueta `<span>`. Sin embargo, ser√≠a mejor si accedi√©ramos al valor del atributo de `data-value`. De esta manera podemos convertir el dato extra√≠do a un `int` sin tener que eliminar una coma.

Puedes tratar un objeto `Tag` como un diccionario. Los atributos HTML son las claves del diccionario. Los valores de los atributos HTML son los valores de las claves del diccionario. As√≠ es como podemos acceder al valor del atributo `data-value`:

```python
print(primeros_votos['data-value'])
```

```bash
23878
```

Convirtamos ese valor a un n√∫mero entero y asign√©moslo a `primeros_votos`:

```python
primeros_votos = int(primeros_votos['data-value'])
```

¬°Eso es todo! Ahora estamos en condiciones de escribir f√°cilmente un script para extraer una sola p√°gina.

## El script para una sola p√°gina.
Antes de reconstruir lo que hemos hecho hasta ahora, debemos asegurarnos de extraer los datos solo de los contenedores que tienen Metascore.

*----- Imagen -------*

Necesitamos agregar una condici√≥n para omitir pel√≠culas sin Metascore.

Usando el inspector de elementos nuevamente, vemos que la secci√≥n Metascore est√° contenida dentro de una etiqueta `<div>`. El atributo `class` tiene dos valores:

- `inline-block `
- `ratings-metascore`. 

El distintivo es claramente el `ratings-metascore`.


*----- Imagen -------*


Podemos usar `find()` para buscar en cada contenedor de pel√≠cula un div que tenga esa marca distintiva. Cuando `find()` no encuentra nada, devuelve un objeto `None`. Podemos usar este resultado en una declaraci√≥n `if` para controlar si una pel√≠cula se scrapea.

Busquemos en la p√°gina web un contenedor de pel√≠culas que no tenga Metascore y veamos qu√© devuelve `find()`.

*Importante: cuando ejecut√© el siguiente c√≥digo, el primer contenedor no ten√≠a Metascore. Sin embargo, este es un objetivo cambiante, porque el n√∫mero de votos cambia constantemente para cada pel√≠cula. Para obtener los mismos resultados que obtuve en la siguiente celda de c√≥digo demostrativo, debe buscar un contenedor que no tenga un Metascore en el momento en que ejecuta el c√≥digo.*


```python
puntaje_mscore_cero = contenedores_pelicula[0].find('div', class_ = 'ratings-metascore')
print(type(puntaje_mscore_cero))
```

```bash
<class 'NoneType'>
```

Ahora juntemos el c√≥digo anterior y comprim√°moslo tanto como sea posible, pero s√≥lo en la medida en que sea f√°cilmente legible. En el siguiente bloque de c√≥digo:

- Declare algunas variables de `lista` para tener algo en qu√© almacenar los datos extra√≠dos.
- Recorra cada contenedor en `contenedores_pelicula` (la variable que contiene los 50 contenedores de pel√≠culas).
- Extraiga los  datos de inter√©s solo si el contenedor tiene un Metascore.
- Instalar el paquete de Python [Pandas](https://pandas.pydata.org/pandas-docs/stable/)

```bash
pip install pandas
```

```python
# Listas para guardar los datos scrapeados
nombres = []
a√±os = []
calificaciones_imdb = []
metascores = []
votos = []

# Extraer dato de un contenedor de pelicula individual
for contenedor in contenedores_pelicula:
# Si la pel√≠cula tiene Metascore, entonces extrae:
    if contenedor.find('div', class_ = 'ratings-metascore') is not None:
        # El nombre
        nombre = contenedor.h3.a.text
        nombres.append(nombre)
        # El a√±o
        a√±o = contenedor.h3.find('span', class_ = 'lister-item-year').text
        a√±os.append(a√±o)
        # La calificaci√≥n IMDB
        imdb = float(contenedor.strong.text)
        calificaciones_imdb.append(imdb)
        # El Metascore
        m_score = contenedor.find('span', class_ = 'metascore').text
        metascores.append(int(m_score))
        # El n√∫mero de votos
        voto = contenedor.find('span', attrs = {'name':'nv'})['data-value']
        votos.append(int(voto))
```

Revisemos los datos recopilados hasta ahora. Pandas nos facilitar√° ver si hemos recopilado nuestros datos correctamente.



```python
import pandas as pd
test_df = pd.DataFrame({'pel√≠cula': nombres,
    'a√±o': a√±o,
    'imdb': calificaciones_imdb,
    'metascore': metascores,
    'votos': votos
})
print(test_df.info())
```


```bash
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 134 entries, 0 to 133
Data columns (total 5 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   pel√≠cula   134 non-null    object 
 1   a√±o        134 non-null    object 
 2   imdb       134 non-null    float64
 3   metascore  134 non-null    int64  
 4   votos      134 non-null    int64  
dtypes: float64(1), int64(2), object(2)
memory usage: 5.4+ KB
None
```

y ahora si reviso los primeros 5 elementos del DataFrame

```python
print(test_df.head(n=10))
```

```bash
                                pel√≠cula         a√±o  imdb  metascore   votos
0                                Hablame   (I) (2022)   7.5         76   24315
1                                Babylon   (I) (2022)   7.1         60  140035
2                                      X  (II) (2022)   6.6         79  141075
3                       Un Vecino Gru√±√≥n       (2022)   7.5         51  119931
4             Avatar: El camino del agua       (2022)   7.6         67  451221
5                                 Batman       (2022)   7.8         72  723010
6                                Top Gun       (2022)   8.3         78  623797
7   Todo en todas partes al mismo tiempo       (2022)   7.8         81  473311
8                             La ballena       (2022)   7.7         60  171725
9                              Tren bala   (I) (2022)   7.3         49  380635
10                               El Men√∫       (2022)   7.2         71  343596
11                                 M3gan       (2022)   6.4         72  118519
12    El gato con botas: El √∫ltimo deseo       (2022)   7.9         73  151006
13                         Purple Hearts       (2022)   6.7         30   45567
14               No te preocupes, cari√±o   (I) (2022)   6.3         48  125158
15                                  Sisu       (2022)   6.9         70   52071
16                      Hasta los Huesos       (2022)   6.8         74   47994
17                     La ciudad perdida       (2022)   6.1         60  138711
18                                 Pearl       (2022)   7.0         76   68405
19           El Tri√°ngulo de la tristeza       (2022)   7.3         63  150714
```

¬°Todo sali√≥ como se esperaba!

Como nota al margen, si ejecuta el c√≥digo desde un pa√≠s donde el ingl√©s no es el idioma principal, es muy probable que obtenga algunos de los nombres de las pel√≠culas traducidos al idioma principal de ese pa√≠s.

Lo m√°s probable es que esto suceda porque el servidor deduce su ubicaci√≥n a partir de su direcci√≥n IP. Incluso si se encuentra en un pa√≠s donde el ingl√©s es el idioma principal, es posible que a√∫n reciba contenido traducido. Esto puede suceder si est√°s usando una VPN mientras realizas las solicitudes `GET`.

Si lo tuyo es el ingl√©s, pasa los siguientes valores al par√°metro de `headers` de la funci√≥n `get()`:

```python
headers = {"Accept-Language": "en-US, en;q=0.5"}
```

Esto comunicar√° al servidor algo como:

*‚ÄúQuiero el contenido ling√º√≠stico en ingl√©s americano (en-US). Si en-US no est√° disponible, entonces otros tipos de ingl√©s (en) tambi√©n estar√≠an bien (pero no tanto como en-US)‚Äù.*

El par√°metro `q` indica el grado en que preferimos un determinado idioma. Si no se especifica, el valor se establece en 1 de forma predeterminada, como en el caso de en-US. Puedes [leer m√°s sobre esto aqu√≠](https://www.rfc-editor.org/rfc/rfc9110.html#name-accept-language).

Ahora comencemos a crear el script para todas las p√°ginas que queremos eliminar.


## El script de varias p√°ginas.
Hacer el scraping para varias p√°ginas es un poco m√°s desafiante. Desarrollaremos nuestro script de una p√°gina haciendo tres cosas m√°s:

1. Haciendo todas las peticiones que queramos desde dentro del bucle.
2. Controlar la velocidad del bucle para evitar bombardear el servidor con peticiones.
3. Supervisar el bucle mientras se ejecuta.

Scrapearemos las primeras 2 p√°ginas de cada a√±o en el intervalo 2000-2022. 2 p√°ginas para cada uno de los 23 a√±os hacen un total de 46 p√°ginas. Cada p√°gina tiene 250 pel√≠culas, por lo que extraeremos datos de 11500 pel√≠culas como m√°ximo. Pero no todas las pel√≠culas tienen un Metascore, por lo que el n√∫mero ser√° menor. Aun as√≠, es muy probable que obtengamos datos de m√°s de 2000 pel√≠culas.


### Cambiando los par√°metros de las URLs

Como se mostr√≥ anteriormente, las URL siguen una cierta l√≥gica a medida que cambian las p√°ginas web.

*----- Imagen -------*

Mientras vamos realizando las peticiones, solo tendremos que variar los valores de solo dos par√°metros de la URL: `release_date` y `start`. Preparemos los valores que necesitaremos para bucle. En nuestro c√≥digo haremos:

- Una lista llamada `paginas` y la rellenaremos con los n√∫meros correspondientes al inicio de cada p√°gina.
- Una lista llamada `url_a√±os` y se completar√° con las cadenas correspondientes a los a√±os 2000-2022.

```python
paginas = (1, 251)
url_a√±os = [str(i) for i in range(2000, 2023)]
```

### Controlando la tasa de rastreo (crawl-rate)
Controlar la velocidad de rastreo es beneficioso para nosotros y para el sitio web que estamos scrapeando. Si evitamos sobrecargar el servidor con decenas de peticiones por segundo, es mucho menos probable que proh√≠ban nuestra direcci√≥n IP. Tambi√©n evitamos interrumpir la actividad del sitio web que scrapeamos al permitir que el servidor tambi√©n responda a las solicitudes de otros usuarios.

Controlaremos la velocidad del bucle usando la [funci√≥n](https://docs.python.org/3/library/time.html?highlight=time%20module#time.sleep) `sleep()` del [m√≥dulo](https://docs.python.org/3/library/time.html?highlight=time%20module#module-time) de `time` de Python. `sleep()` pausar√° la ejecuci√≥n del bucle durante una cantidad espec√≠fica de segundos.

Para imitar el comportamiento humano, variaremos la cantidad de tiempo de espera entre solicitudes usando la [funci√≥n](https://docs.python.org/3/library/random.html?highlight=random%20module#random.randint) `randint()` del [m√≥dulo](https://docs.python.org/3/library/random.html?highlight=random%20module#module-random) `random` de Python. `randint()` genera aleatoriamente n√∫meros enteros dentro de un intervalo espec√≠fico.

```python
from time import sleep
from random import randint

for _ in range(0, 5):
    print('Pin pra')
    sleep(randint(1,4))
```

Por lo pronto, solo importemos estas dos funciones en nuestro c√≥digo para evitar la saturaci√≥n en el siguiente bloque de c√≥digo que contiene nuestro bucle principal de sue√±o.

### Monitoreando el bucle mientras contin√∫a

Dado que estamos scrapeando 46 p√°ginas, ser√≠a bueno si pudi√©ramos encontrar una manera de monitorear el proceso de scrapeo mientras a√∫n contin√∫a. Esta caracter√≠stica es definitivamente opcional, pero puede resultar muy √∫til en el proceso de prueba y depuraci√≥n. Adem√°s, cuanto mayor sea el n√∫mero de p√°ginas, m√°s √∫til ser√° el seguimiento. Si va a scrapear cientos o miles de p√°ginas web en una sola ejecuci√≥n de c√≥digo, dir√≠a que esta caracter√≠stica se vuelve imprescindible.

Para nuestro script, usaremos esta funci√≥n y monitorearemos los siguientes par√°metros:

- La **frecuencia (velocidad) de las peticiones**, por lo que nos aseguramos de que nuestro programa no sobrecargue el servidor.
- La **cantidad de peticiones**, para que podamos detener el ciclo en caso de que se exceda la cantidad de solicitudes esperadas.
- El **[c√≥digo de estado HTTP](https://es.wikipedia.org/wiki/Anexo:C%C3%B3digos_de_estado_HTTP)** de nuestras solicitudes, para asegurarnos de que el servidor env√≠e las respuestas adecuadas.

Para obtener un valor de frecuencia, dividiremos la cantidad de solicitudes por el tiempo transcurrido desde la primera solicitud. Esto es similar a calcular la velocidad de un autom√≥vil: dividimos la distancia por el tiempo necesario para recorrerla. Primero experimentemos con esta t√©cnica de monitoreo a peque√±a escala. Lo siguiente que haremos ser√°:

- Establecer una hora de inicio utilizando la funci√≥n `time()` del m√≥dulo `time` y asignar el valor a `tiempo_inicio`.
- Asignar 0 a la variable `peticiones` que usaremos para contar el n√∫mero de solicitudes.
- Iniciar un bucle y luego con cada iteraci√≥n:
    - Simular una solicitud.
    - Incrementar el n√∫mero de peticiones en 1.
    - Pausae el bucle durante un intervalo de tiempo de entre 8 y 15 segundos.
    - Calcule el tiempo transcurrido desde la primera solicitud y asignar el valor a `tiempo_transcurrido`.
    - Imprimir el n√∫mero de solicitudes y la frecuencia.

As√≠ lucir√° nuestra prueba de concepto:

```python
from time import time
from random import randint

tiempo_inicio = time()
peticiones = 0

for _ in range(5):
    peticiones += 1
    sleep(randint(1,3))
    tiempo_transcurrido = time() - tiempo_inicio
    print(f'Petici√≥n: {peticiones}; Frecuencia: {peticiones/tiempo_transcurrido} peticiones/s')
```

```bash
Petici√≥n: 1; Frecuencia: 0.49947650463238624 peticiones/s
Petici√≥n: 2; Frecuencia: 0.4996998027377252 peticiones/s
Petici√≥n: 3; Frecuencia: 0.5995400143227362 peticiones/s
Petici√≥n: 4; Frecuencia: 0.4997272043465967 peticiones/s
Petici√≥n: 5; Frecuencia: 0.4543451628627026 peticiones/s
```

Dado que vamos a realizar 46 solicitudes, nuestro trabajo se ver√° un poco desordenado a medida que se acumule el resultado. Para evitarlo, borraremos el resultado despu√©s de cada iteraci√≥n y lo reemplazaremos con informaci√≥n sobre la solicitud m√°s reciente. 

Este paso solo sirve si est√°s probando desde un Jupyter notebook, sino, puedes ignorarlo.

Para hacer eso usaremos la funci√≥n `clear_output()` del [m√≥dulo](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.clear_output) `display` de IPython. Estableceremos el par√°metro de espera de `clear_output()` en True para esperar y reemplazar la salida actual hasta que aparezca alguna salida nueva.

```python
from time import time

from IPython.display import clear_output

tiempo_inicio = time()
peticiones = 0

for _ in range(5):
    peticiones += 1
    sleep(randint(1,3))
    tiempo_transcurrido = time() - tiempo_inicio
    print(f'Petici√≥n: {peticiones}; Frecuencia: {peticiones/tiempo_transcurrido} peticiones/s')
clear_output(wait = True)
```

```
Petici√≥n: 5; Frecuencia: 0.6240351700607663 peticiones/s
```

El resultado anterior es el que ver√° una vez que se haya ejecutado el ciclo. As√≠ es como se ve mientras est√° funcionando

*----- GIF -------*

Para monitorear el c√≥digo de estado, configuraremos el programa para que nos avise si hay algo mal. Una solicitud exitosa se indica mediante un c√≥digo de estado de 200. Usaremos la funci√≥n `warn()` del m√≥dulo de `warnings` para generar una advertencia si el c√≥digo de estado no es 200.

```python
from warnings import warn
warn("Simulaci√≥n de una advertencia")
```

Y como resultado, veremos:

```bash
<stdin>:1: UserWarning: Simulaci√≥n de una advertencia
```

Elegimos una advertencia en lugar de romper el ciclo porque existe una buena posibilidad de que extraigamos suficientes datos, incluso si algunas de las solicitudes fallan. Solo romperemos el ciclo si el n√∫mero de solicitudes es mayor de lo esperado.

*Nota:* Adicionalmente a lo que sugiere el tutorial original sobre el monitoreo, instal√© [ipython-autotime](https://pypi.org/project/ipython-autotime/) para ver el tiempo de ejecuci√≥n de cada celda en mi jupyter notebook.

```bash
pip install ipython-autotime
```

Luego de esto, solo es colocar `%load_ext autotime` en la primera celda del cuaderno y listo.

### Juntando todo

¬°Ahora reconstruyamos todo lo que hemos hecho hasta ahora! Luego de los import que ya hicimos de los m√≥dulos `time`, `warn`y `random`, continuaremos con:

- Iterar sobre la lista `url_a√±os` para variar el par√°metro `release_date` de la URL.
- Para cada elemento en `url_a√±os`, recorra la lista de p√°ginas para variar el par√°metro de `start` de la URL.
- Realice las solicitudes `GET` dentro del bucle de p√°ginas.
- Pause el bucle durante un intervalo de tiempo de entre 8 y 15 segundos.
- Supervise cada solicitud como se analiz√≥ anteriormente.
- Lanze una advertencia para c√≥digos de estado distintos de 200.
- Rompa el ciclo si el n√∫mero de solicitudes es mayor de lo esperado.
- Convierta el contenido HTML de la `respuesta` en un objeto `BeautifulSoup`.
- Extraiga todos los contenedores de pel√≠culas de este objeto `BeautifulSoup`.
- Recorra todos estos contenedores.
- Extraiga los datos si un contenedor tiene un Metascore.

```python
# Listas para guardar los datos scrapeados
nombres = []
a√±os = []
calificaciones_imdb = []
metascores = []
votos = []

# Preparacion para monitorear las iteraciones
tiempo_inicio = time()
peticiones = 0


for a√±o in url_a√±os:
    for pagina in paginas:

        url = f'https://www.imdb.com/search/title/?release_date={a√±o}-01-01,{a√±o}-12-31'
        url += f'&sort=num_votes,desc&count=250&start={pagina}&ref_=adv_nxt'

        # Hacer la peticion
        respuesta = get(url)

        # Pausar el ciclo
        sleep(randint(8,15))

        # Monitorear las peticiones
        peticiones += 1
        tiempo_transcurrido = time() - tiempo_inicio
        print(f'Petici√≥n: {peticiones}; Frecuencia: {peticiones/tiempo_transcurrido} peticiones/s')
        # clear_output(wait = True)

        # Throw a warning for non-200 status codes
        if respuesta.status_code != 200:
            warn(f'A√±o: {a√±o};   Petici√≥n: {peticiones}; Status code: {respuesta.status_code}')

        # Rompe el ciclo si el n√∫mero de peticiones es muy mayor que el esperado
        if peticiones > 47:
            warn('N√∫mero de peticiones fue mayor al esperado.')
            break

        # Parsea el contenido de la petici√≥n con BeautifulSoup
        sopa_html = BeautifulSoup(respuesta.text, 'html.parser')

        # Selecciona todos los 250 contenedores de pel√≠culas de una sola p√°gina
        contenedores_pelicula = sopa_html.find_all('div', class_ = 'lister-item mode-advanced')

        # Por cada pel√≠cula de estas 250
        for contenedor in contenedores_pelicula:
            # Si la pel√≠cula tiene Metascore, entonces extrae:
            if contenedor.find('div', class_ = 'ratings-metascore') is not None:
                
                # El nombre
                nombre = contenedor.h3.a.text
                nombres.append(nombre)

                # El a√±o
                a√±o = contenedor.h3.find('span', class_ = 'lister-item-year').text
                a√±os.append(a√±o)

                # La calificaci√≥n IMDB
                imdb = float(contenedor.strong.text)
                calificaciones_imdb.append(imdb)

                # El Metascore
                m_score = contenedor.find('span', class_ = 'metascore').text
                metascores.append(int(m_score))

                # El n√∫mero de votos
                voto = contenedor.find('span', attrs = {'name':'nv'})['data-value']
                votos.append(int(voto))
```

```bash
Petici√≥n: 44; Frecuencia: 0.06678366415302274 peticiones/s
```
¬°Que bien! El scrapeado parece haber funcionado perfectamente. El script dur√≥ 10min 35s.

Ahora combinemos los datos en un `DataFrame` de pandas para examinar lo que hemos logrado extraer. Si todo es como se esperaba, podemos continuar con la limpieza de los datos para dejarlos listos para el an√°lisis.

## Examinando los datos scrapeados
Ahora lo siguiente ser√°:

- Fusionar los datos en un `DataFrame` de pandas.
- Descargar los datos en un archivo con formato `.csv` (¬°por si la moscas!)
- Imprimir informaci√≥n sobre el `DataFrame` reci√©n creado.
- Mostrar las primeras entradas.

```python
df = pd.DataFrame({
    'pel√≠cula': nombres,
    'a√±o': a√±os,
    'imdb': calificaciones_imdb,
    'metascore': metascores,
    'votos': votos
})

df.to_csv('calificaciones_peliculas_raw.csv')

print(df.info())
print(df.head(15))
```

```bash
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3980 entries, 0 to 3979
Data columns (total 6 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   Unnamed: 0  3980 non-null   int64  
 1   pel√≠cula    3980 non-null   object 
 2   a√±o         3980 non-null   object 
 3   imdb        3980 non-null   float64
 4   metascore   3980 non-null   int64  
 5   votos       3980 non-null   int64  
dtypes: float64(1), int64(3), object(2)
memory usage: 186.7+ KB
None
```

```bash
    Unnamed: 0                    pel√≠cula     a√±o  imdb  metascore    votos
0            0                   Gladiator  (2000)   8.5         67  1555914
1            1                     Memento  (2000)   8.4         83  1282227
2            2          Cerdos y diamantes  (2000)   8.2         55   883470
3            3         Requiem for a Dream  (2000)   8.3         71   871723
4            4             American Psycho  (2000)   7.6         64   672128
5            5                       X-Men  (2000)   7.3         64   632513
6            6                   Cast Away  (2000)   7.8         73   617566
7            7                 Unbreakable  (2000)   7.3         62   431171
8            8      Mission: Impossible II  (2000)   6.1         59   365811
9            9            Meet the Parents  (2000)   7.0         73   346377
10          10  O Brother, Where Art Thou?  (2000)   7.7         69   322841
11          11       Gone in Sixty Seconds  (2000)   6.5         35   288235
12          12               Almost Famous  (2000)   7.9         90   286334
13          13                 The Patriot  (2000)   7.2         63   286082
14          14             Wo hu cang long  (2000)   7.9         94   277167
```

El resultado de `info()` muestra que recopilamos datos de m√°s de 2000 pel√≠culas. Tambi√©n podemos ver que no hay ning√∫n valor `null` en nuestro conjunto de datos.

He verificado las calificaciones de estas primeras 10 pel√≠culas en el sitio web de IMDB. Todos estaban en lo cierto. Quiz√°s quieras hacer t√∫ lo mismo.

Podemos proceder con seguridad a limpiar los datos.


## Limpiando los datos scrapeados
Limpiaremos los datos extra√≠dos con dos objetivos en mente: 

1. Dibujar la distribuci√≥n de las calificaciones de IMDB y Metascore
2. Compartir el conjunto de datos. 

En consecuencia, nuestra limpieza de datos consistir√° en:

- Reordenar las columnas.
- Limpiar la columna de `a√±o` y convertir los valores a n√∫meros enteros.
- Verificar los valores de calificaci√≥n extremos para determinar si todas las calificaciones est√°n dentro de los intervalos esperados.
- Normalizar uno de los tipos de calificaciones (o ambos) para generar un histograma comparativo.

Ya que en este punto es necesario graficar y analizar los datos, te recomiendo instalar jupyter notebook en caso de que no lo hayas hecho.

```bash
pip install notebook
```

Comencemos reordenando las columnas:

```python
df = df[['pel√≠cula', 'a√±o', 'imdb', 'metascore', 'votos']]
df.head()
```

```bash
    pel√≠cula    a√±o imdb    metascore   votos
0   Gladiator   (2000)  8.5 67  1555914
1   Memento (2000)  8.4 83  1282227
2   Cerdos y diamantes  (2000)  8.2 55  883470
3   Requiem for a Dream (2000)  8.3 71  871723
4   American Psycho (2000)  7.6 64  672128
```

Ahora convirtamos todos los valores de la columna de a√±o a n√∫meros enteros.

En este momento todos los valores son del tipo de `object`. Para evitar `ValueErrors` durante la conversi√≥n, queremos que los valores est√©n compuestos √∫nicamente por n√∫meros del 0 al 9.

Examinemos los valores √∫nicos de la columna de a√±o. Esto nos ayuda a hacernos una idea de lo que podr√≠amos hacer para realizar las conversiones que queremos. Para ver todos los valores √∫nicos, usaremos el m√©todo `unique()`:

```python
df['a√±o'].unique()
```

```bash
array(['(2000)', '(I) (2000)', '(2001)', '(I) (2001)', '(2002)',
       '(I) (2002)', '(2002 Video)', '(2003)', '(I) (2003)', '(2004)',
       '(I) (2004)', '(2005)', '(I) (2005)', '(2006)', '(I) (2006)',
       '(III) (2006)', '(II) (2006)', '(2007)', '(I) (2007)',
       '(II) (2007)', '(2008)', '(I) (2008)', '(2009)', '(I) (2009)',
       '(II) (2009)', '(2010)', '(I) (2010)', '(II) (2010)', '(2011)',
       '(I) (2011)', '(IV) (2011)', '(2012)', '(I) (2012)', '(II) (2012)',
       '(2013)', '(I) (2013)', '(II) (2013)', '(2014)', '(I) (2014)',
       '(II) (2014)', '(III) (2014)', '(2015)', '(I) (2015)',
       '(II) (2015)', '(VI) (2015)', '(III) (2015)', '(I) (2016)',
       '(2016)', '(II) (2016)', '(IX) (2016)', '(V) (2016)', '(2017)',
       '(I) (2017)', '(II) (2017)', '(III) (2017)', '(2018)',
       '(I) (2018)', '(III) (2018)', '(II) (2018)', '(I) (2019)',
       '(2019)', '(II) (2019)', '(III) (2019)', '(2020)', '(I) (2020)',
       '(II) (2020)', '(IV) (2020)', '(V) (2020)', '(2021)', '(I) (2021)',
       '(II) (2021)'], dtype=object)
```

Contando desde el final hacia el principio, podemos ver que los a√±os siempre se ubican del quinto car√°cter al segundo. Usaremos el m√©todo `.str()` para seleccionar solo ese intervalo. Tambi√©n convertiremos el resultado a un n√∫mero entero usando el m√©todo `astype()`.

```python
df.loc[:, 'a√±o'] = df['a√±o'].str[-5:-1].astype(int)
```

Si hacemos esto tal cual, encontraremos un error en la conversi√≥n de los datos, ya que hay registros con la palabra `Video` dentro de ellas. Para ello, lo primero que tenemos que hacer es remover esa cadena de texto. Aprovechar√© para quitar tambi√©n el par√©ntesis, por lo que ahora mi rebanada llegar√° hasta el cuarto caracter de derecha a izquierda.

```python
df.loc[:, 'a√±o'] = df['a√±o'].str.strip(' Video)').str[-4:]
```

Como vemos ahora, los datos ya no tienen el texto `Video)` dentro de ellos.

```bash
array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
       2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021],
      dtype=object)
```

Ahora si, convirtamos a n√∫meros enteros:

```python
df = df.astype({'a√±o': int})
```

Visualicemos los primeros 5 valores de la columna del a√±o para una verificaci√≥n r√°pida. Tambi√©n podemos ver el tipo de valores en la √∫ltima l√≠nea de la salida:

```python
df['a√±o'].head(5)
```

```bash
0    2000
1    2000
2    2000
3    2000
4    2000
Name: a√±o, dtype: int64
```

Ahora comprobaremos los valores m√≠nimos y m√°ximos de cada tipo de calificaci√≥n. Podemos hacer esto muy r√°pidamente usando el [m√©todo](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) `describe()` de pandas. Cuando se aplica en un `DataFrame`, este m√©todo devuelve varias estad√≠sticas descriptivas para cada columna num√©rica del `DataFrame. En la siguiente l√≠nea de c√≥digo seleccionamos solo aquellas filas que describen los valores m√≠nimo y m√°ximo, y solo aquellas columnas que describen calificaciones de IMDB y Metascores.

```python
df.describe().loc[['min', 'max'], ['imdb', 'metascore']]
```

```bash
    imdb    metascore
min  1.5    7.0
max  9.0    100.0
```

No hay valores at√≠picos inesperados.

De los valores anteriores, puede ver que las dos calificaciones tienen escalas diferentes. Para poder trazar las dos distribuciones en un solo gr√°fico, tendremos que llevarlas a la misma escala. Normalicemos la columna `imdb` a una escala de 100 puntos.

Multiplicaremos cada calificaci√≥n de IMDB por 10 y luego haremos una verificaci√≥n r√°pida mirando las primeras 3 filas:


```python
df['n_imdb'] = df['imdb'] * 10
print(df.head(3))
```

```bash
    Unnamed: 0  pel√≠cula    a√±o imdb    metascore   votos   n_imdb
0   0   Gladiator   2000    8.5 67  1555914 85.0
1   1   Memento 2000    8.4 83  1282227 84.0
2   2   Cerdos y diamantes  2000    8.2 55  883470  82.0
```

¬°Genial! Ahora estamos en condiciones de guardar este conjunto de datos localmente, para poder compartirlo con otros m√°s f√°cilmente. Ya lo compart√≠ p√∫blicamente en este repositorio de GitHub de la comunidad. Hay otros lugares donde puedes compartir un conjunto de datos, como Kaggle o Dataworld.

As√≠ que guard√©moslo:

```python
df.to_csv('calificaciones_peliculas.csv')
```

Finalmente, grafiquemos las distribuciones!


## Graficando y analizando las distribuciones
En la siguiente celda de c√≥digo dentro de jupyter notebook vamos a:

- Instalar `matplotlib`

```bash
pip install matplotlib
```
- Importar el subm√≥dulo `matplotlib.pyplot`.
- Correr la magia de Jupyter `%matplotlib` para activar el modo matplotlib de Jupyter y luego agregar `inline` para que nuestros gr√°ficos se muestren dentro del cuaderno.
- Crea un objeto `figure` con 3 ejes.
- Dibujar la distribuci√≥n de cada calificaci√≥n no normalizada en un eje individual `ax`.
- Dibujar las distribuciones normalizadas de las dos calificaciones en el mismo eje `ax`.

```python
import matplotlib.pyplot as plt
%matplotlib inline

fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (16,4))
ax1, ax2, ax3 = fig.axes
ax1.hist(df['imdb'], bins = 10, range = (0,10)) # bin range = 1
ax1.set_title('IMDB rating')

ax2.hist(df['metascore'], bins = 10, range = (0,100)) # bin range = 10
ax2.set_title('Metascore')

ax3.hist(df['n_imdb'], bins = 10, range = (0,100), histtype = 'step', label='n_imdb')
ax3.hist(df['metascore'], bins = 10, range = (0,100), histtype = 'step', label='metascore')
ax3.legend(loc = 'upper left')
ax3.set_title('The Two Normalized Distributions')

for ax in fig.axes:
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
plt.show()
```

![Distribuciones pel√≠culas](/img/posts/2023/tutorial-de-web-scraping-y-beautifulsoup/distribuciones.png)

Comenzando con el [histograma](https://datavizcatalogue.com/methods/histogram.html) de IMDB, podemos ver que la mayor√≠a de calificaciones est√°n entre 6 y 8. Hay pocas pel√≠culas con una calificaci√≥n mayor a 8, y menos a√∫n con una calificaci√≥n menor a 4. Esto indica que es raro observar tanto pel√≠culas muy buenas como pel√≠culas muy malas.

La distribuci√≥n de las calificaciones de Metascore se asemeja a una [distribuci√≥n normal](https://www.mathsisfun.com/data/standard-normal-distribution.html): la mayor√≠a de las calificaciones son promedio y alcanzan un m√°ximo de aproximadamente 50. A partir de este pico, las frecuencias disminuyen gradualmente hacia valores de calificaci√≥n extremos. Seg√∫n esta distribuci√≥n, efectivamente hay menos pel√≠culas muy buenas y muy malas, pero no tan pocas como indican las valoraciones de IMDB.

En el gr√°fico comparativo, queda m√°s claro que la distribuci√≥n de IMDB est√° muy sesgada hacia la parte m√°s alta de las calificaciones promedio, mientras que las calificaciones de Metascore parecen tener una distribuci√≥n mucho m√°s equilibrada.

*¬øCu√°l podr√≠a ser la raz√≥n de ese sesgo en la distribuci√≥n de IMDB?* 

Una hip√≥tesis es que muchos usuarios tienden a utilizar un m√©todo binario para evaluar pel√≠culas. Si les gusta la pel√≠cula, le dan un 10. Si no les gusta, le dan una calificaci√≥n muy peque√±a o no se molestan en calificarla. Este es un problema interesante que vale la pena explorar con m√°s detalle.


## Siguientes pasos
Hemos recorrido un largo camino desde solicitar el contenido de una sola p√°gina web hasta analizar las calificaciones de m√°s de 2000 pel√≠culas. Ahora deber√≠a saber c√≥mo raspar muchas p√°ginas web con la misma estructura HTML y URL.

Para aprovechar lo que hemos aprendido, estos son algunos de los siguientes pasos a considerar:

- Extraiga datos para diferentes intervalos de tiempo y p√°gina.
- Raspe datos adicionales sobre las pel√≠culas.
- Encuentre un sitio web diferente para raspar algo que le interese. 
    - Por ejemplo, podr√≠a recopilar datos sobre [computadoras port√°tiles](https://listado.mercadolibre.com.co/port%C3%A1tiles#D[A:port%C3%A1tiles]) para ver c√≥mo var√≠an los precios con el tiempo.

---
excerpt: Un breve tutorial sobre web scraping usando beautiful soup. Es una traducci√≥n de un art√≠culo de dataquest que ya no se encuentra disponible
---
pub_date: 2023-08-15
---
render_tdc: yes
